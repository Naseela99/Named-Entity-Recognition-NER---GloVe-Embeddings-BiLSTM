# -*- coding: utf-8 -*-
"""CSCI544_HW4(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XTmWn3yvj1l8WqA4duYAi7fRH2XuZD0T
"""

import pandas as pd
#from google.colab import drive
import torch
from torch import nn
import numpy as np
import torch
from torch.utils.data import DataLoader,Dataset
from sklearn.utils.class_weight import compute_class_weight
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from tqdm import tqdm
from sklearn.metrics import  classification_report
import gzip



class TrainDatasetTask1(Dataset):
    def __init__(self,train_features,labels):
        self.train_features = train_features
        self.labels = labels
    def __len__(self):
        return len(self.train_features)
    def __getitem__(self,index):
    
        return self.train_features[index],self.labels[index]
        
class TestDatasetTask1(Dataset):
    def __init__(self,sentences):
        self.sentences = sentences
       

    def __len__(self):
        return len(self.sentences)
    def __getitem__(self,index):
    
        return self.sentences[index]

class TrainDatasetTask2(Dataset):
    def __init__(self,sentences, cases, labels):
        self.sentences = sentences
        self.cases = cases
        
        self.labels = labels

    def __len__(self):
        return len(self.sentences)
    def __getitem__(self,index):
    
        return (self.sentences[index], self.cases[index]), self.labels[index]
    

class TestDatasetTask2(Dataset):
    def __init__(self,sentences, cases):
        self.sentences = sentences
        self.cases = cases
     

    def __len__(self):
        return len(self.sentences)
    def __getitem__(self,index):
    
        return self.sentences[index],self.cases[index]

class Preprocessor:
    def __init__(self, vocab=None, tag_map=None):
        self.vocab = vocab
        self.tag_map = tag_map

    def load_vocab(self, df, unk_th=2):
        counts = df.value_counts(subset="word", dropna=False)

        counts = pd.DataFrame(counts, columns=["count"]).reset_index()

        counts.loc[counts["count"]<unk_th, "word"] = "<UNK>"

        vocab = (counts.groupby("word", as_index=False).sum().
                    sort_values("count", ascending=False)).reset_index(drop=True).reset_index()[["word", "index", "count"]]
        vocab['index']+=1

        vocab = vocab.set_index("word").drop(labels="count", axis=1)["index"].to_dict()
        vocab['<<PAD>>'] = 0
        self.vocab = vocab

    def load_tag_map(self, df):
        counts_tag = df.value_counts(subset="tag", dropna=False)

        counts_tag = pd.DataFrame(counts_tag, columns=["count"]).reset_index()



        tag_map = (counts_tag.groupby("tag", as_index=False).sum().
                sort_values("count", ascending=False)).reset_index(drop=True).reset_index()[["tag", "index", "count"]]

        tag_map = tag_map.set_index("tag").drop(labels="count", axis=1)["index"].to_dict()
        self.tag_map = tag_map

    def process(self, df, training=True, use_cases=True):
        assert self.vocab is not None, "vocab is not created. please call `self.load_vocab(df)`."
        if not training:
            assert self.tag_map is not None, "tag_map is not created. please call `self.tag_map(df)`."


        df = df.reset_index(drop=True)
        startindices = df[df["index"]=='1'].index
        endindices = startindices-1

        startindices = startindices.tolist()
        endindices = endindices.tolist()
        endindices.pop(0)
        endindices.append(len(df)-1) 

        sentences = []
        if training:
            ner = []
        if use_cases:
            cases = []   

        for st, en in zip(startindices, endindices):
            words = df.loc[st:en,'word'].values.tolist()

            sentences.append(torch.LongTensor([self.vocab.get(word, self.vocab['<UNK>']) for word in words]))
            if training:
                ner.append(torch.LongTensor([self.tag_map[t] for t in df.loc[st:en, "tag"].values.tolist()])) 
            if use_cases:
                cases.append(torch.tensor([(int(word!=word.lower())-0.5)*2 for word in words]))

        rv = [sentences]
        if use_cases:
            rv.append(cases)
        if training:
            rv.append(ner)


        return rv[0] if len(rv)==1 else rv

class BiLSTMTask1(nn.Module):
    def __init__(self, vocab_size,n_classes,emb_dim, hidden_dim, output_dim, num_layers, dropout, pad_idx):
        super(BiLSTMTask1, self).__init__()
        self.hidden_dim = hidden_dim
        self.emb_dim = emb_dim
        self.num_layers = num_layers
        self.dropout = dropout
        
        self.embedding = nn.Embedding(vocab_size,emb_dim,padding_idx=pad_idx)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, dropout=dropout, bidirectional=True,batch_first = True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout_layer = nn.Dropout(dropout)
        
        self.classifier = nn.Linear(output_dim,n_classes)
        
        self.elu = nn.ELU()
        
    def forward(self, x):
        device = next(self.parameters()).device
        x = x.to(device)

        embedded = self.dropout_layer(self.embedding(x)) 
        lstm_output, _ = self.lstm(embedded) 
        output = self.elu(self.fc(lstm_output))

        output = self.dropout_layer(output)
        output = nn.functional.log_softmax(self.classifier(output), dim=-1)
        
        output = output.transpose(1,2)
        
    
        return output

class BiLSTMTask2(nn.Module):
    def __init__(self, vocab_size, n_classes, emb_dim, emb_matrix, hidden_dim, output_dim, num_layers, dropout, pad_idx):
        super(BiLSTMTask2, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.dropout = dropout
        

        self.embedding = nn.Embedding(vocab_size,emb_dim, padding_idx=pad_idx)
        self.embedding.weight = nn.Parameter(torch.from_numpy(emb_matrix.astype('float32')))

        self.lstm = nn.LSTM(101, hidden_dim, num_layers=num_layers, dropout=dropout, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.dropout_layer = nn.Dropout(dropout)
        
        self.classifier = nn.Linear(output_dim,n_classes)
        
        self.elu = nn.ELU()

        
    def forward(self, x):
        device = next(self.parameters()).device

        sen, case = x
        sen = sen.to(device)
        case = case.to(device)
        embedded = self.embedding(sen)
        batch_size = embedded.shape[0]

        embedded = torch.cat((embedded, case.view(batch_size, -1, 1)), dim=-1)
        embedded = self.dropout_layer(embedded)
        lstm_output, _ = self.lstm(embedded) 
        output = self.elu(self.fc(lstm_output))

        output = self.dropout_layer(output)
        output = nn.functional.log_softmax(self.classifier(output), dim=-1)
        
        output = output.transpose(1,2)
        
    
        return output

def training(model, train_loader, dev_loader, criterion, optimizer, scheduler, n_epochs):
    for epoch in range(n_epochs):
        train_loss = 0.0
        val_loss = 0.0
        device = next(model.parameters()).device

        with tqdm(total = len(train_loader)) as p_bar:
            model.train()

            for data, target in train_loader:
                target = target.to(device)

                optimizer.zero_grad()
                output = model(data)

                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

                curr_loss = loss.item()*target.size(0)
                train_loss+=curr_loss


                p_bar.set_postfix(loss = curr_loss)
                p_bar.set_description(f"Epoch: {epoch+1}")
                p_bar.update()
            train_loss = train_loss/len(train_loader.dataset)

            model.eval()
            with torch.no_grad():
                for data, target in dev_loader:
                    target = target.to(device)
                    output = model(data)

                    loss = criterion(output, target)
                    
                    curr_loss = loss.item()*target.size(0)
                    val_loss+=curr_loss
            
                val_loss = val_loss/len(dev_loader.dataset)
            scheduler.step(val_loss)
        print('Epoch: {} \tValidation Loss: {:.6f}\t Train Loss: {:.6f} LR: {}'.format(epoch+1,val_loss, train_loss, optimizer.param_groups[0]['lr']))

def predict(model, data):
    model.eval()
    all_predictions = []
    with torch.no_grad():
        for d in tqdm(data):
            predictions = model(d)
            _, predict = torch.max(predictions.data, 1)
            all_predictions.append(predict.to(torch.device("cpu")).numpy()[0].tolist())
    
    return all_predictions

def get_pred_df(model, df, preprocessor, use_cases):
    data = preprocessor.process(df, use_cases=use_cases, training=False)

    if use_cases:
        data = list(zip(*data))
        data = [(x.unsqueeze(0), y.unsqueeze(0)) for x, y in data]
    else:
        data = [x.unsqueeze(0) for x in data]
    tags = predict(model, data)
    reverse_tag_map = {v:k for k,v in preprocessor.tag_map.items()}
    tags = [reverse_tag_map[x] for y in tags for x in y]
    df["pred_tag"] = tags

    return df

def testing(model, weights_path, test_path, dev_path, preprocessor, use_cases):

    weights = torch.load(weights_path, map_location=torch.device('cpu'))
    model.load_state_dict(weights)
    model.to(device)

    test_df = load_df(test_path, train=False)
    test_df = test_df.dropna()
    test_df = get_pred_df(model, test_df, preprocessor, use_cases)

    dev_df = load_df(dev_path, train=True)
    dev_df = dev_df.dropna()
    dev_df = get_pred_df(model, dev_df, preprocessor, use_cases)

    return dev_df, test_df

def load_df(path, train=True):
    data_train = []
    with open(path,'r') as f:
        data_train = f.readlines()

    train_data = []
    for data in data_train:
        train_data.append(data.strip('\n').split(' '))
    columns = ["index", "word"]
    if train:
        columns.append("tag")
    df = pd.DataFrame(train_data, columns=columns)
    return df

num_workers = 2
batch_size = 32

def get_embedding_matrix(path, vocab):

    with gzip.open(path,'rb') as f:
        glove_emb = f.readlines()

    emb = dict()

    for e in glove_emb:
        e = e.decode()
        e = e.rstrip('\n')
        e = e.split(' ')
        w,g = e[0], list(map(float,e[1:]))
        emb[w] = g

    reverse_vocab = {v:k for k, v in vocab.items()}
    emb_matrix = [np.zeros(100)]
    for i in range(1, len(reverse_vocab)):
        ## generate a random embedding for every unknown word
        unk_emb = np.random.randn(100)*0.5
        emb_matrix.append(emb.get(reverse_vocab[i].lower(), unk_emb))

    emb_matrix = np.array(emb_matrix)
    return emb_matrix

def train_task(model, train_loader, dev_loader, n_epochs):
    class_weights = [1,0.7,1,1,1,1,1,1,1] 

    class_weights = torch.tensor(class_weights,dtype=torch.float)

    criterion = nn.CrossEntropyLoss(ignore_index=-1, weight=class_weights.to(device))
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay = 1e-6, momentum=0.99)

    scheduler = ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2)

    training(model, train_loader, dev_loader, criterion, optimizer, scheduler, n_epochs=n_epochs)

def train_task2(model, train_loader, dev_loader, n_epochs):
    class_weights = [1,0.7,1,1,1,1,1,1,1] 

    class_weights = torch.tensor(class_weights,dtype=torch.float)

    criterion = nn.CrossEntropyLoss(ignore_index=-1, weight=class_weights.to(device))
    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay = 1e-6, momentum=0.99)

    scheduler = ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2)

    training(model, train_loader, dev_loader, criterion, optimizer, scheduler, n_epochs=n_epochs)

def save_to_files(dev_df,test_df,task):
  val = dev_df.values.tolist()
  res1 = ''
  res2 = ''
  # print(val[0])

  for v in val:
    v_out = v.copy()
    v_out.pop(2)
    # if v[3] not in list(idx_word_tag.values()):
    #   break
    if v[0] == '1':
      res1 +='\n'+" ".join(v)
      res2+= '\n'+" ".join(v_out)
      
    else:
      res1+=" ".join(v)
      res2+=" ".join(v_out)
    res1+='\n'
    res2+='\n'
  result1 = 'idx word gold pred\n' + res1[1:]
  result2 = res2[1:]

  val = test_df.values.tolist()
  res_test = ''

  for v in val:
    # if v[2] not in list(idx_word_tag.values()):
    #   break
    if v[0] == '1':
      res_test +='\n'+" ".join(v)
    else:
      res_test+=" ".join(v)
    res_test+='\n'
  
  result_test = res_test[1:]


  
  if task == 1:
    # with open('dev1_test.out', 'w') as f:
    #   f.write(result1)
   # !cp dev1_test.out "drive/My Drive/"
    with open('dev1.out', 'w') as f:
      f.write(result2)
    #!cp dev1.out "drive/My Drive/"
    # with open('test1.out', 'w') as f:
    #   f.write(result_test)
    #!cp test1.out "drive/My Drive/"
  else:
    # with open('dev2_test.out', 'w') as f:
    #   f.write(result1)
    #!cp dev2_test.out "drive/My Drive/"
    with open('dev2.out', 'w') as f:
      f.write(result2)
    #!cp dev2.out "drive/My Drive/"
    # with open('test2.out', 'w') as f:
    #   f.write(result_test)
    #!cp test2.out "drive/My Drive/"

if __name__ == '__main__':
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    df = load_df("data/train", train=True)
    df = df.dropna()

    preprocessor = Preprocessor()
    preprocessor.load_vocab(df)
    preprocessor.load_tag_map(df)

    sentences, cases, ner = preprocessor.process(df, use_cases=True, training=True)

    sentences = nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=0)
    cases = nn.utils.rnn.pad_sequence(cases, batch_first=True, padding_value=0)
    ner = nn.utils.rnn.pad_sequence(ner, batch_first=True, padding_value=-1)

    dev_df = load_df("data/dev", train=True)
    dev_df = dev_df.dropna()

    dev_sentences, dev_cases, dev_ner = preprocessor.process(dev_df, use_cases=True, training=True)

    dev_sentences = nn.utils.rnn.pad_sequence(dev_sentences, batch_first=True, padding_value=0)
    dev_cases = nn.utils.rnn.pad_sequence(dev_cases, batch_first=True, padding_value=0)
    dev_ner = nn.utils.rnn.pad_sequence(dev_ner, batch_first=True, padding_value=-1)

    # Task 1
    train_data = TrainDatasetTask1(sentences, ner)
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=num_workers)

    dev_data = TrainDatasetTask1(dev_sentences, dev_ner)
    dev_loader = torch.utils.data.DataLoader(dev_data, batch_size=batch_size,num_workers=num_workers)

    modeltask1 = BiLSTMTask1(vocab_size=len(preprocessor.vocab), n_classes=len(preprocessor.tag_map),
                            emb_dim=100, hidden_dim=256, output_dim=128, num_layers=1,
                            dropout=0.33, pad_idx=0)
    #modeltask1 = modeltask1.to(device)
    #train_task(modeltask1, train_loader, dev_loader, n_epochs=55)
    #torch.save(modeltask1.state_dict(), "blstm1.pt")
    

    dev_df_task1, test_df_task1 = testing(modeltask1, weights_path="blstm1.pt", 
                                          test_path="data/test", 
                                          dev_path="data/dev", 
                                          preprocessor=preprocessor, use_cases=False)
    
    save_to_files(dev_df_task1,test_df_task1,task=1)



    ## Task 2
    train_data = TrainDatasetTask2(sentences, cases, ner)
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,num_workers=num_workers)

    dev_data = TrainDatasetTask2(dev_sentences, dev_cases, dev_ner)
    dev_loader = torch.utils.data.DataLoader(dev_data, batch_size=batch_size,num_workers=num_workers)


    emb_matrix = get_embedding_matrix("glove.6B.100d.gz", preprocessor.vocab)


    modeltask2 = BiLSTMTask2(vocab_size=len(preprocessor.vocab), n_classes=len(preprocessor.tag_map),
                            emb_dim=100,emb_matrix=emb_matrix.copy(),
                             hidden_dim=256, output_dim=128, num_layers=1, dropout=0.33, pad_idx=0)
    #modeltask2 = modeltask2.to(device)
    #train_task2(modeltask2, train_loader, dev_loader, n_epochs=120)
    #torch.save(modeltask2.state_dict(), "blstm2.pt")

   


    dev_df_task2, test_df_task2 = testing(modeltask2, weights_path="blstm2.pt",
                                          test_path="data/test",
                                          dev_path="data/dev",
                                          preprocessor=preprocessor, use_cases=True)
    
    save_to_files(dev_df_task2,test_df_task2,task=2)



